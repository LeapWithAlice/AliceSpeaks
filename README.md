# AliceSpeaks™
  The AliceSpeaks™ application is an iOS native build using emerging technologies. These include the Apple Vision and Machine Learning frameworks to identify items. Once done the application will then allow user to tap on the speech button to have the identified item stated out loud in english and translated in a user selected language of choice.

# How It Works

**- Once the application is installed on the device the user simply opens the application.**

<a href="https://s3.amazonaws.com/lwa-hosted-image/ezgif-2-9cd68f6148.gif" title="made at imgflip.com"/></a>

**- In the bottom right corner of the app you will notice something called 'Recognition Process'. Here the camera view along with the Machine Learning model will give you the top results for what it believes it is looking at.**

<a href="https://imgflip.com/gif/2bvqft"><img src="https://i.imgflip.com/2bvqft.gif" title="made at imgflip.com"/></a>

**- When you are focused on an item you may tap on the screen in the location you feel fit and an augmented 3D label will be placed in that location with the word of the top result as seen in the 'Recognition Process'.**

<a href="https://s3.amazonaws.com/lwa-hosted-image/ezgif-2-9f82edd102.gif"/></a>


# Technologies Used

### Apple Vision Framework

```The Vision framework performs face and face landmark detection, text detection, barcode recognition, image registration, and general feature tracking. Vision also allows the use of custom Core ML models for tasks like classification or object detection.```

### Apple Machine Learning Model

```Core ML 2 lets you integrate a broad variety of machine learning model types into your app. In addition to supporting extensive deep learning with over 30 layer types, it also supports standard models such as tree ensembles, SVMs, and generalized linear models. Because it’s built on top of low level technologies like Metal and Accelerate, Core ML seamlessly takes advantage of the CPU and GPU to provide maximum performance and efficiency. You can run machine learning models on the device so data doesn't need to leave the device to be analyzed.```

### Google Translate API

```The Google Cloud Translation API can dynamically translate text between thousands of language pairs. The Cloud Translation API lets websites and programs integrate with the translation service programmatically. The Google Translation API is part of the larger Cloud Machine Learning API family.```




## Where Can You Find Us?

* [Webpage](https://leapwithalice.io)
* [Alice iOS App](https://itunes.apple.com/us/app/leap-with-alice/id1369587027?platform=iphone&preserveScrollPosition=true&platform=iphone&platform=iphone&platform=iphone#platform/iphone&platform=iphone&platform=iphone&platform=iphone)
* [Alice Android App](https://play.google.com/store/apps/details?id=com.lwa.demo)
* [Facebook](https://www.facebook.com/LeapWithAlice/?ref=br_rs)
* [Twitter](https://twitter.com/LeapWithAlice) 
* [Telegram](https://t.me/LWAlice)
* [Youtube](https://www.youtube.com/channel/UCrrw59HelHtZcLsNwUMCsIA?view_as=subscriber) 
* [Github](https://github.com/AlfonsoMorales/Leap-With-Alice-Demo)
* [Medium](https://medium.com/@LeapWithAlice)
